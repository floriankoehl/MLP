import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

# load iris data
data = load_iris()
X = data.data
y = data.target

# split into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# convert numpy arrays to float32 tensors
X_train = torch.tensor(X_train, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.long)
X_test = torch.tensor(X_test, dtype=torch.float32)
y_test = torch.tensor(y_test, dtype=torch.long)



import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# create a dataframe for easier plotting
df = pd.DataFrame(X, columns=data.feature_names)
df['species'] = y

# map target integers to species names for better legend
species_map = {0: 'setosa', 1: 'versicolor', 2: 'virginica'}
df['species'] = df['species'].map(species_map)

# create a pairplot to see data spread
sns.pairplot(df, hue='species')
plt.title("Iris Data Visualization")
plt.savefig('iris_data_spread.png')
print("Data visualization saved to iris_data_spread.png")




class SimpleMLP(nn.Module):
    def __init__(self):
        super(SimpleMLP, self).__init__()
        # input layer to hidden layer (4 features -> 16 hidden nodes)
        self.fc1 = nn.Linear(4, 16)
        # hidden layer to output layer (16 hidden nodes -> 3 classes)
        self.fc2 = nn.Linear(16, 3)
        self.relu = nn.ReLU()

    def forward(self, x):
        # pass through first layer and apply activation
        x = self.fc1(x)
        x = self.relu(x)
        # pass through output layer
        x = self.fc2(x)
        return x



# initialize the model
model = SimpleMLP()

# loss function for classification
criterion = nn.CrossEntropyLoss()

# optimizer with learning rate
optimizer = optim.Adam(model.parameters(), lr=0.01)



# list to store loss values
loss_history = []

epochs = 100

for epoch in range(epochs):
    optimizer.zero_grad()

    outputs = model(X_train)
    loss = criterion(outputs, y_train)

    loss.backward()
    optimizer.step()

    # save loss for plotting
    loss_history.append(loss.item())

    if (epoch + 1) % 10 == 0:
        print(f'epoch {epoch+1}, loss: {loss.item():.4f}')



# plot the training loss
plt.figure(figsize=(8, 5))
plt.plot(range(1, epochs + 1), loss_history, label='training loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training Loss Over Time')
plt.legend()
plt.grid(True)

# save the plot to a file
plt.savefig('Training_loss_plot.png')
print("Loss plot saved to training_loss_plot.png")


from sklearn.metrics import accuracy_score

# turn off gradients for validation
with torch.no_grad():
    test_outputs = model(X_test)

    # get the class with the highest score
    _, predicted = torch.max(test_outputs, 1)

    # calculate accuracy using sklearn (alternative to manual calc)
    accuracy = accuracy_score(y_test, predicted)
    print(f'final test accuracy: {accuracy:.4f}')
